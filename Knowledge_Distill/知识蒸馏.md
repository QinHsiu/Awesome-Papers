# 知识蒸馏

### 1.自知识蒸馏

### arxiv22 Self-Knowledge Distillation with Bidirectional Chronological Augmentation of Transformer for Sequential Recommendation

![image-20221210185303411](C:\Users\QinHsiu\AppData\Roaming\Typora\typora-user-images\image-20221210185303411.png)

##### code:[link](https://github.com/juyongjiang/BiCAT)

### MM22 **Modality-Aware Contrastive Instance Learning with Self-Distillation for Weakly-Supervised Audio-Visual Violence Detection**

![image-20221210190603437](C:\Users\QinHsiu\AppData\Roaming\Typora\typora-user-images\image-20221210190603437.png)

##### code:[link](https://github.com/JustinYuu/MACIL_SD)



### 2. 离线蒸馏

### arxiv22 **Contrastive Supervised Distillation for Continual Representation Learning**

![image-20221210185619578](C:\Users\QinHsiu\AppData\Roaming\Typora\typora-user-images\image-20221210185619578.png)



### 3. 在线多模型蒸馏

### arxiv22 **Prune Your Model Before Distill It**

![image-20221210185744436](C:\Users\QinHsiu\AppData\Roaming\Typora\typora-user-images\image-20221210185744436.png)

##### code:[link](https://github.com/ososos888/prune-then-distill)

### AAAI22 **SAIL: Self-Augmented Graph Contrastive Learning**

![image-20221210185405674](C:\Users\QinHsiu\AppData\Roaming\Typora\typora-user-images\image-20221210185405674.png)

### CVPR22 **Decoupled Knowledge Distillation**

![image-20221210190125801](C:\Users\QinHsiu\AppData\Roaming\Typora\typora-user-images\image-20221210190125801.png)

##### code:[link](https://github.com/megviiresearch/mdistiller)

### ICLR22 UNSUPERVISED SEMANTIC SEGMENTATION BY DISTILLING FEATURE CORRESPONDENCES

![image-20221210190347510](C:\Users\QinHsiu\AppData\Roaming\Typora\typora-user-images\image-20221210190347510.png)



### NeurIPS22 **Knowledge Distillation from A Stronger Teacher**

![image-20221210190726111](C:\Users\QinHsiu\AppData\Roaming\Typora\typora-user-images\image-20221210190726111.png)

##### code:[link]( https://github.com/hunto/DIST_KD)

### WSDM22 **Unbiased Knowledge Distillation for Recommendation**

![image-20221210191227480](C:\Users\QinHsiu\AppData\Roaming\Typora\typora-user-images\image-20221210191227480.png)

##### code:[link](https://github.com/chengang95/UnKD)

### SIGIR22 **On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation**

![image-20221210190932719](C:\Users\QinHsiu\AppData\Roaming\Typora\typora-user-images\image-20221210190932719.png)

##### code:[link](https://github.com/xiaxin1998/OD-Rec)

### arxiv22 **A Generalized Supervised Contrastive Learning Framework**

![image-20221210191756433](C:\Users\QinHsiu\AppData\Roaming\Typora\typora-user-images\image-20221210191756433.png)

##### code:[link](https://t.ly/yuUO)

### arxiv22 **data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language**

![image-20221210201200879](C:\Users\QinHsiu\AppData\Roaming\Typora\typora-user-images\image-20221210201200879.png)

##### code:[link](www.github.com/pytorch/fairseq/tree/master/examples/data2vec)

### 4. 联邦学习

### arxiv22 **FedX: Unsupervised Federated Learning with Cross Knowledge Distillation**

![image-20221210190032520](C:\Users\QinHsiu\AppData\Roaming\Typora\typora-user-images\image-20221210190032520.png)