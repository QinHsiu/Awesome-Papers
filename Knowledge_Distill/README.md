- [[CSD][ICIAP 21]Contrastive Supervised Distillation for Continual Representation Learning](https://arxiv.org/abs/2205.05476)
- [[UnKD][WSSDM 22]Unbiased Knowledge Distillation for Recommendation](https://arxiv.org/abs/2211.14729)
- [[SKD][SIGIR 22]On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation](https://arxiv.org/abs/2204.11091)
- [[DISTKD][NeurIPS 22]Knowledge Distillation from A Stronger Teacher](https://arxiv.org/abs/2205.10536)
- [[MACILSD][MM 22]Modality-Aware Contrastive Instance Learning with Self-Distillation for Weakly-Supervised Audio-Visual Violence Detection](https://arxiv.org/abs/2207.05500)
- [[STEGO][ICLR 22]Unsupervised Semantic Segmentation by Distilling Feature Correspondences](https://arxiv.org/abs/2203.08414)
- [[DKD][CVPR 22]Decoupled Knowledge Distillation](https://arxiv.org/abs/2203.08679)
- [[Data2Vec][ICML 22]data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555)
- [[FedX][ECCV 22]FedX: Unsupervised Federated Learning with Cross Knowledge Distillation](https://arxiv.org/abs/2207.09158)
- [[Prune][ECCV 22]Prune Your Model Before Distill It](https://arxiv.org/abs/2109.14960)
- [[GenSCL][Arxiv 22]A Generalized Supervised Contrastive Learning Framework](https://arxiv.org/abs/2206.00384)
- [[SAIL][AAAI 22]SAIL: Self-Augmented Graph Contrastive Learning](https://arxiv.org/abs/2009.00934)
- [[SimKD][CVPR 22]Knowledge Distillation with the Reused Teacher Classifi](https://arxiv.org/abs/2203.14001)
- [[BiCAT][Arxiv 22]Self-Knowledge Distillation with Bidirectional Chronological Augmentation of Transformer for Sequential Recommendation](https://arxiv.org/abs/2112.06460)
- [[CoBERT][ICASSP 23]CoBERT: SELF-SUPERVISED SPEECH REPRESENTATION LEARNING THROUGH CODE REPRESENTATION LEARNING](https://arxiv.org/abs/2210.04062)
- [[DPK][WSDM 23]Learning to Distill Graph Neural Networks](http://www.shichuan.org/doc/144.pdf)
  
