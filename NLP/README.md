- 论文
  - Dial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue Embeddings EMNLP2022 [Paper](https://arxiv.org/abs/2210.15332v1) [Note](https://juejin.cn/post/7184244058171113509)
  - [[COCODR][EMNLP22]COCO-DR: Combating Distribution Shifts in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning](https://arxiv.org/abs/2210.15212)
  - [[CodeRetriver][EMNLP 22]CodeRetriever: Large-scale Contrastive Pre-training for Code Search](https://arxiv.org/abs/2201.10866)
  - [[DuReader][EMNLP 22]DuReaderretrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine](https://arxiv.org/abs/2203.10232)
  - [[mHFN][EMNLP 22]Recovering Gold from Black Sand: Multilingual Dense Passage Retrieval with Hard and False Negative Samples](https://aclanthology.org/2022.emnlp-main.730.pdf)
  - [[RetroMAE][EMNLP 22]etroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder](https://arxiv.org/abs/2205.12035)

  - [[Distill][ACL 23]Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://arxiv.org/abs/2305.02301)
  - [[BLIP2][Arxiv 23]BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)
 
  - [[Generative Agent][Arxiv 23]Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442)
  - [[CAMEL][Arxiv 23]CAMEL: Communicative Agents for “Mind” Exploration of Large Scale Language Model Society](https://arxiv.org/abs/2303.17760)
  - [[OpenAGI][Arxiv 23]OpenAGI: When LLM Meets Domain Experts](https://arxiv.org/abs/2304.04370)
  - [[Pythia][Arxiv 23]Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373)
  - [[SelfDebug][Arxiv 23]Teaching Large Language Models to Self-Debug](https://arxiv.org/abs/2304.05128)
  - [[LLM Survey][Arxiv 23]A Survey on Multimodal Large Language Models](https://arxiv.org/abs/2306.13549)

- 量化
  - [[Int4][Arxiv 23]Training Transformers with 4-bit Integers](https://arxiv.org/pdf/2306.11987.pdf)
  - [[Int8]](https://mp.weixin.qq.com/s/_JirS9knfTlta0qOzo3i6A)
    
- Pretrain
  - [[AdaptBERT][PMLR 19]Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)
  - [[TER][ACL 21]Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)
  - [[TransTailor][AAAI 21]TransTailor: Pruning the Pre-trained Model for Improved Transfer Learning](https://arxiv.org/abs/2103.01542)
  - [[WSC][EMNLP 21]The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)
  - [[GLUE][EMNLP 21]Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning](https://arxiv.org/abs/2109.05687)
  - [[LM][ACL 22]Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models](https://arxiv.org/abs/2106.13353)
  - [[LoRA][ICLR 22]LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
  - [[ptuning][ACL 22]P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/abs/2110.07602)
  - [[BitFit][ACL 22]BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199)
  - [[PPT][ACL 22]PPT: Pre-trained Prompt Tuning for Few-shot Learning](https://arxiv.org/abs/2109.04332)
  - [[unify][ICLR 22]TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING](https://arxiv.org/abs/2110.04366)
  - [[LMaas][ICML 22]Black-Box Tuning for Language-Model-as-a-Service](https://arxiv.org/abs/2201.03514)
  - [[STEP][KDD 22]Pre-training Enhanced Spatial-temporal Graph Neural Network for Multivariate Time Series Forecasting](https://arxiv.org/abs/2206.09113)
  
- LLM训练优化
    - [[LAWA][Arxiv 23]Understanding the Effectiveness of Early Weight Averaging for Training Large Language Models](https://arxiv.org/pdf/2306.03241.pdf)
    - [[LOMO][Arxiv 23]Full Parameter Fine-tuning for Large Language Models with Limited Resources](https://arxiv.org/abs/2306.09782)

- 解读
  - [AutoCoT](https://mp.weixin.qq.com/s/9hnjmV-A8SE3_EzQeg85xA)
  - [Prompt](https://mp.weixin.qq.com/s/g1NKoqUhrtwgstDM0GoGxA)
  - [LLM](https://mp.weixin.qq.com/s/nxbNueiW6TEdjsQItmnO9A)
  - [LLM + 多模态](https://mp.weixin.qq.com/s/Q8SITBzTxlrFDkUleVZHiw)
  - [参数有效性](https://mp.weixin.qq.com/s/sOPxL_Lq4lg3tbIsmEoMuw)
  - [重新思考transformer](https://mp.weixin.qq.com/s/UzxkuZOMWPPFJCgbk9TGwg)
  - [百度-搜索召回调研](https://mp.weixin.qq.com/s/W2FA4VRX8oG8dUn6z8IQ2Q)
