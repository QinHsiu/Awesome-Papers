- 论文
  - Dial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue Embeddings EMNLP2022 [Paper](https://arxiv.org/abs/2210.15332v1) [Note](https://juejin.cn/post/7184244058171113509)
  - [[COCODR][EMNLP22]COCO-DR: Combating Distribution Shifts in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning](https://arxiv.org/abs/2210.15212)
  - [[CodeRetriver][EMNLP 22]CodeRetriever: Large-scale Contrastive Pre-training for Code Search](https://arxiv.org/abs/2201.10866)
  - [[DuReader][EMNLP 22]DuReaderretrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine](https://arxiv.org/abs/2203.10232)
  - [[mHFN][EMNLP 22]Recovering Gold from Black Sand: Multilingual Dense Passage Retrieval with Hard and False Negative Samples](https://aclanthology.org/2022.emnlp-main.730.pdf)
  - [[RetroMAE][EMNLP 22]etroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder](https://arxiv.org/abs/2205.12035)

  - [[Distill][ACL 23]Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://arxiv.org/abs/2305.02301)
  - [[BLIP2][Arxiv 23]BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)
  - [[CodeChain][Arxiv 23]CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules](https://arxiv.org/pdf/2310.08992.pdf)
  - [[In-Context Pretraining][Arxiv 23]IN-CONTEXT PRETRAINING: LANGUAGE MODELING BEYOND DOCUMENT BOUNDARIES](https://arxiv.org/pdf/2310.10638.pdf)
  - [[Self-Reg][Arxiv 23]SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION](https://arxiv.org/pdf/2310.11511.pdf)
  - [[GFM][Arxiv 23]Towards Graph Foundation Models: A Survey and Beyond](https://arxiv.org/pdf/2310.11829.pdf)
  - [[3D-GPT][Arxiv 23]3D-GPT: Procedural 3D Modeling with Large Language Models](https://arxiv.org/pdf/2310.12945.pdf)
  - [[MusicAgent][Arxiv 23]MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models](https://arxiv.org/pdf/2310.11954.pdf)
  - [[Llemma][Arxiv 23]Llemma: An Open Language Model For Mathematics](https://arxiv.org/pdf/2310.10631.pdf)
  - [[Table-GPT][Arxiv 23]Table-GPT: Table-tuned GPT for Diverse Table Tasks](https://arxiv.org/pdf/2310.09263.pdf)
 
  - [[Generative Agent][Arxiv 23]Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442)
  - [[CAMEL][Arxiv 23]CAMEL: Communicative Agents for “Mind” Exploration of Large Scale Language Model Society](https://arxiv.org/abs/2303.17760)
  - [[OpenAGI][Arxiv 23]OpenAGI: When LLM Meets Domain Experts](https://arxiv.org/abs/2304.04370)
  - [[Pythia][Arxiv 23]Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373)
  - [[SelfDebug][Arxiv 23]Teaching Large Language Models to Self-Debug](https://arxiv.org/abs/2304.05128)
  - [[LLM Survey][Arxiv 23]A Survey on Multimodal Large Language Models](https://arxiv.org/abs/2306.13549)
  - [[LLM Self-translate][Arxiv2023]](https://mp.weixin.qq.com/s/KDL61ArerCFEirN53_b5UQ)

- 量化
  - [[Int4][Arxiv 23]Training Transformers with 4-bit Integers](https://arxiv.org/pdf/2306.11987.pdf)
  - [[Int8]](https://mp.weixin.qq.com/s/_JirS9knfTlta0qOzo3i6A)
  - [[BitNet][Arxiv 23]BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/pdf/2310.11453.pdf)
    
- Pretrain
  - [[AdaptBERT][PMLR 19]Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)
  - [[TER][ACL 21]Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)
  - [[TransTailor][AAAI 21]TransTailor: Pruning the Pre-trained Model for Improved Transfer Learning](https://arxiv.org/abs/2103.01542)
  - [[WSC][EMNLP 21]The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)
  - [[GLUE][EMNLP 21]Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning](https://arxiv.org/abs/2109.05687)
  - [[LM][ACL 22]Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models](https://arxiv.org/abs/2106.13353)
  - [[LoRA][ICLR 22]LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
  - [[ptuning][ACL 22]P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/abs/2110.07602)
  - [[BitFit][ACL 22]BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199)
  - [[PPT][ACL 22]PPT: Pre-trained Prompt Tuning for Few-shot Learning](https://arxiv.org/abs/2109.04332)
  - [[unify][ICLR 22]TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING](https://arxiv.org/abs/2110.04366)
  - [[LMaas][ICML 22]Black-Box Tuning for Language-Model-as-a-Service](https://arxiv.org/abs/2201.03514)
  - [[STEP][KDD 22]Pre-training Enhanced Spatial-temporal Graph Neural Network for Multivariate Time Series Forecasting](https://arxiv.org/abs/2206.09113)
  
- LLM训练优化
    - [[StructGPT][EMNLP 23]StructGPT: A General Framework for Large Language Model to Reason over Structured Data](https://arxiv.org/pdf/2305.09645.pdf)   
    - [[LAWA][Arxiv 23]Understanding the Effectiveness of Early Weight Averaging for Training Large Language Models](https://arxiv.org/pdf/2306.03241.pdf)
    - [[LOMO][Arxiv 23]Full Parameter Fine-tuning for Large Language Models with Limited Resources](https://arxiv.org/abs/2306.09782)
    - [[LongNet][Arxiv 23]LONGNET: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/pdf/2307.02486.pdf)
    

 - 自动对齐
   - [Self-Alignment with Instruction Backtranslation](https://arxiv.org/pdf/2308.06259.pdf)

- 评估
    - [[InstructBLIP][Arxi 23]Evaluating Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2305.10355) [Code](https://github.com/RUCAIBox/POPE)
    - [[SmartMoE][USENIX ATC 23]SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization](https://www.usenix.org/system/files/atc23-zhai.pdf) [Code](https://github.com/zms1999/SmartMoE)

- 推理
    - [[LLM使用tensorRT进行推理]](https://mp.weixin.qq.com/s/Sv1B1WbjgeL4mlehsyfLFg)
      
- 解读
  - [AutoCoT](https://mp.weixin.qq.com/s/9hnjmV-A8SE3_EzQeg85xA)
  - [Prompt](https://mp.weixin.qq.com/s/g1NKoqUhrtwgstDM0GoGxA)
  - [LLM](https://mp.weixin.qq.com/s/nxbNueiW6TEdjsQItmnO9A)
  - [[Large Language Models for User Interest Journeys]](https://mp.weixin.qq.com/s/WLGAhLq61FxMTWKCpSJ0yg)
  - [[Hydra]](https://github.com/extremebird/Hydra) [note](https://mp.weixin.qq.com/s/xYSU3An7m7pHwwFOuJMFyg)
  - [LLM + 多模态](https://mp.weixin.qq.com/s/Q8SITBzTxlrFDkUleVZHiw)
  - [参数有效性](https://mp.weixin.qq.com/s/sOPxL_Lq4lg3tbIsmEoMuw)
  - [重新思考transformer](https://mp.weixin.qq.com/s/UzxkuZOMWPPFJCgbk9TGwg)
  - [百度-搜索召回调研](https://mp.weixin.qq.com/s/W2FA4VRX8oG8dUn6z8IQ2Q)
  - [多智能体+强化学习](https://mp.weixin.qq.com/s/C_bNa42FdR5xLRcbSLXSCg)
  - [大模型中的注意力机制](https://mp.weixin.qq.com/s/ioVRA1Y4R9a4Zu1tLKdUSA)
  - [[LLM4Rec]Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Recommendation via LLM](https://mp.weixin.qq.com/s/xODzKgqYhAQ-jFGT7NwaNA)
  - [[LLM4Rank]](https://mp.weixin.qq.com/s/Tc2_NT0VzWvUOFcE18ssZw)
  - [[LLM4Vis][EMNLP 2023]LLM4Vis: Explainable Visualization Recommendation using ChatGPT](https://mp.weixin.qq.com/s/sBYFF1hgDCfbmwbU17q_9Q)
  - [大模型性能优化](https://mp.weixin.qq.com/s/2nuKyb1d1L6ISTTSteUuyA)
  - [[DATAug4NLP]](https://mp.weixin.qq.com/s/qB0ZyU391-JpsN-2FsXd2g)
  - [[LLM4Rec][WSDM 2024]](https://mp.weixin.qq.com/s/SbmkaAOvE9ucCIKEYx5JsQ)
  - [[检索大模型][ACL 23]](https://mp.weixin.qq.com/s/33a0D1MLiC5bvte0KUOXfA)
  - [[ChatGPT&Agent]](https://mp.weixin.qq.com/s/gxHAPmzd2yzJE6ih4cHijw)
  - [[SFT]](https://mp.weixin.qq.com/s/3RIBzuVlK0qHbO_Q04s-cw)
  - [[大模型推理]](https://mp.weixin.qq.com/s/vRpRmHXGjHjgsr20ktTh5A)
  - [[LLM中的transformer]](https://mp.weixin.qq.com/s/0FWCtXObchkmDXdxdtOsLg)
  - [[LLM对于任务型对话的作用]](https://mp.weixin.qq.com/s/kkWrEM9moH0DCraXBY4QHA)
    
- 资源
  - [LLM](http://yqli.tech/page/aigc_llm.html)
