## TTS_vits
##### è®°å½•ä¸€äº›å…³äºåœ¨vitsæ¨¡å‹ä¸Šè¿›è¡Œæ”¹è¿›çš„TTSæ¨¡å‹ï¼ŒåŒ…æ‹¬è®ºæ–‡æ–¹æ³•è§£è¯»ã€ä»£ç èµ„æºç­‰ï¼›

### 21å¹´ä»¥åŠä¹‹å‰çš„æ–‡ç« 
-ã€ŠEfficient Neural Audio Synthesisã€‹ ICML2018
  - ä½¿ç”¨å•å±‚RNNæ­é…softmaxæ¥åˆæˆè¯­éŸ³ï¼Œå…¶åˆ›æ–°ç‚¹ä¸»è¦æœ‰ä»¥ä¸‹å‡ ç‚¹
  - 1.æå‡ºä½¿ç”¨å•å±‚RNNçš„ç½‘ç»œæ¶æ„ï¼Œä½¿ç”¨äº†åŒå±‚softmaxï¼Œå¯ä»¥å®æ—¶ç”Ÿæˆ24k 16bitçš„è¯­éŸ³ï¼›
  - è¿™é‡Œå°†åŸå§‹è¾“å‡ºçœ‹ä½œä¸¤éƒ¨åˆ†ï¼Œé«˜å­—èŠ‚ï¼ˆç»†ç²’åº¦çš„ï¼‰å’Œä½å­—èŠ‚ï¼ˆç²—ç²’åº¦çš„ï¼‰ï¼Œç»™å®šå‰ä¸€æ—¶åˆ»çš„ç²—ç²’åº¦çš„è¡¨ç¤º$c_{t-1}$å’Œç»†ç²’åº¦è¡¨ç¤º$$f_{t-1}$$ï¼Œä½¿ç”¨å…¬å¼å¾—åˆ°ä¸‹ä¸€ä¸ªæ—¶åˆ»çš„ç²—ç²’åº¦çš„è¡¨ç¤º$$c_{t}$$ï¼Œç„¶åä½¿ç”¨è¿™ä¸‰ä¸ªæ•°æ®ä½œä¸ºè¾“å…¥ç”Ÿæˆä¸‹ä¸€ä¸ªæ—¶åˆ»çš„ç»†ç²’åº¦çš„è¡¨ç¤º$$c_{t}$$ï¼Œè¿™é‡Œå¯ä»¥ä½¿ç”¨çš„è¡¨è¾¾å¼å¦‚ä¸‹ï¼š
  $$p(c_{t})=f_{c}(c_{t-1},f_{t-1}),p(f_{t})=f_{f}(c_{t-1},f_{t-1},c_{t})$$
  - 2.é‡‡ç”¨äº†æƒé‡å‰ªæçš„æŠ€æœ¯ï¼Œå®ç°äº†96%çš„ç¨€ç–æ€§ï¼›
  éšæœºåˆå§‹åŒ–ä¸€ä¸ªçŸ©é˜µï¼Œæ¯è®­ç»ƒä¸€å®šçš„stepså°±å°†çŸ©é˜µä¸­æœ€å°çš„kä¸ªå…ƒç´ ç½®æ¢ä¸º0.ï¼Œkå¯ä»¥æ˜¯åŠ¨æ€çš„ï¼ˆé€’å¢çš„ï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯é¢„å…ˆè®¾ç½®å¥½çš„ï¼›
  - 3.é«˜å¹¶è¡Œåº¦ç”Ÿæˆè¯­éŸ³ç®—æ³•ï¼ŒæŠŠå¾ˆé•¿çš„åºåˆ—ç”ŸæˆæŠ˜å æˆè‹¥å¹²ä¸ªçŸ­çš„åºåˆ—ï¼Œæ¯ä¸€ä¸ªçŸ­åºåˆ—åŒæ—¶ç”Ÿæˆï¼Œæé«˜é•¿åºåˆ—çš„ç”Ÿæˆé€Ÿåº¦ï¼›
å¯¹äºç»™å®šçš„128bitçš„æ•°æ®ï¼Œå¯ä»¥çœ‹ä½œ[1,2,...,128]ï¼Œç°åœ¨å°†å…¶è°ƒæ•´ä¸º[8*16]å¤§å°çš„çŸ©é˜µï¼Œæ•°æ®ä»ä¸‹å¾€ä¸Šï¼Œä»å·¦è‡³å³ä¾æ¬¡æ’åˆ—ï¼Œç°åœ¨å¯ä»¥è¾¾åˆ°ä¸€ä¸ªä¸‹é™é‡‡æ ·çš„ç­–ç•¥ï¼Œå…¶é¦–å…ˆç”Ÿæˆçš„æ˜¯æœ€ä¸‹é¢çš„æ•°æ®[1,9,17,...]ï¼Œåœ¨ç”Ÿæˆ37ã€76ã€107çš„æ—¶å€™å¯ä»¥å®ç°å¹¶è¡Œï¼Œæœ€åå°†ç”Ÿæˆçš„æ•°æ®å˜æ¢ä¸ºåŸå§‹å¤§å°ï¼Œä¹Ÿå³å®ç°äº†é«˜åº¦å¹¶è¡Œå¿«é€Ÿç”Ÿæˆè¯­éŸ³çš„ç›®çš„ï¼›

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/1802.08435.pdf
  - codeï¼šhttps://github.com/fatchord/WaveRNN
  - å­¦ä¹ èµ„æ–™ï¼šhttps://www.jianshu.com/p/b3019f2773ed

-ã€ŠStyle Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesisã€‹ICML2018
  - 1.å¼•å…¥ä¸€ä¸ªreference encoderæ¥å­¦ä¹ å…·ä½“çš„style embeddingï¼Œåœ¨inferenceé˜¶æ®µç›´æ¥ä½¿ç”¨è®­ç»ƒå¥½çš„reference encoderç¼–ç éŸ³é¢‘å¹¶å°†å¾—å‡ºçš„style embeddingåŠ å…¥åˆ°ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œè¿™é‡Œæœ‰å‡ ä¸ªé—®é¢˜ï¼š
    - style embeddingçš„shapeæ˜¯ä¸æ˜¯é¢„å…ˆå®šä¹‰çš„ï¼Œä¹Ÿå°±æ˜¯styleçš„é£æ ¼æ•°ç›®æ˜¯ä¸æ˜¯ä¸€å¼€å§‹å°±ç¡®å®šäº†ï¼ˆè¿™é‡Œå¦‚æœäº‹å…ˆç¡®å®šå¯ä»¥ä¸å¯ä»¥ä½¿ç”¨k-mensèšç±»æ¥å­¦ä¹ å…·ä½“çš„style embeddingï¼‰ï¼Ÿ
    - psï¼šè¿™é‡Œçš„style embeddingæœ€åä½œä¸ºä¸‹æ¸¸ä»»åŠ¡ä¸­attentionçš„queryèå…¥åˆ°æ¨¡å‹ä¸­ï¼Œè¿™é‡Œçš„styleæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­æå–å‡ºæ¥çš„ï¼Œå…¶é£æ ¼æ•°ç›®ä¸€å¼€å§‹å°±ç¡®å®šäº†ï¼›è¿™é‡Œçš„reference encoderä½¿ç”¨çš„æ˜¯CNN+GRUï¼ŒSTLä½¿ç”¨çš„æ˜¯multi-head self-attentionï¼Œä¸€ä¸ªGST=ã€‹ä¸€ä¸ªreference encoder+ä¸€ä¸ªSTLæ¨¡å‹ï¼ˆå¤šå¤´æ³¨æ„åŠ›æ¨¡å‹ï¼‰ï¼›
  $$Attention(Q,K,V)=softmax(\frac{Q\cdotÂ K^{T}}{\sqrt{d/h}})\cdotÂ V$$ï¼Œtransformerä¸­encoder å’Œdecoderè¿æ¥éƒ¨åˆ†æ˜¯å°†ecoderçš„è¾“å‡ºä¸­çš„keyå’Œvalueä¼ é€’åˆ°decoderä¸­ï¼Œç„¶åä¸decoderä¸­ç¬¬ä¸€ä¸ªblockä¸­çš„valueè¿›è¡Œä¸Šå¼è®¡ç®—è·å¾—ï¼›
    - 2.æ”¹è¿›æ€è·¯ï¼š
    - 0).å°†æ³¨æ„åŠ›å¾—åˆ†å“ªé‡Œå°±æ˜¯è®¡ç®—å…·ä½“çš„styleç±»å‹å“ªé‡Œæ˜¯ä¸æ˜¯å¯ä»¥æ›¿æ¢ä¸ºæ— ç›‘ç£èšç±»ï¼Œç›´æ¥ä½¿ç”¨èšç±»ä¸­å¿ƒè¡¨ç¤ºå…·ä½“çš„é£æ ¼ç±»å‹ï¼›
    - 1).æ˜¯å¦å¯ä»¥å¼•å…¥å¯¹æ¯”å­¦ä¹ ï¼Œè®©å­¦åˆ°çš„æ¨¡å‹è§åˆ°å¤šç§å¤šæ ·çš„embeddingè¡¨ç¤ºï¼›

    - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/1803.09017.pdf
    - codeï¼šhttps://github.com/KinglittleQ/GST-Tacotron
    - å­¦ä¹ èµ„æ–™ï¼šhttps://zhuanlan.zhihu.com/p/380487271
- ã€ŠOne-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalizationã€‹InterSpeech 2019
  - è¯¥è®ºæ–‡ç›®æ ‡æ˜¯è§£å†³å¹¶è¡Œè¯­æ–™å¸¦æ¥çš„ä¸¤ä¸ªé—®é¢˜ï¼š
  - ï¼ˆ1ï¼‰è®­ç»ƒæ•°æ®è¦æ±‚è¾ƒä¸ºä¸¥æ ¼ï¼ˆå¹¶è¡Œæ•°æ®ï¼‰ï¼›
  - ï¼ˆ2ï¼‰åªèƒ½è½¬æ¢å¤„äºè®­ç»ƒé›†å†…çš„éŸ³è‰²ï¼›
  - æœ¬æ–‡é€šè¿‡instance normalizationç­‰æŠ€æœ¯è¿›è¡ŒéŸ³è‰²å’Œå†…å®¹åˆ†ç¦»ï¼Œç„¶åé‡ç»„éŸ³è‰²å’Œå†…å®¹ï¼Œè¾¾åˆ°ç”Ÿäº§ç›®æ ‡éŸ³è‰²çš„ç›®çš„ï¼›å…³äºéŸ³è‰²è½¬æ¢çš„æ–¹æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼ˆåŸºäºå¹¶è¡Œè¯­æ–™çš„æœ‰ç›‘ç£å­¦ä¹ å’ŒåŸºäºéå¹¶è¡Œè¯­æ–™çš„æ— ç›‘ç£å­¦ä¹ ï¼‰ï¼Œå…¶æ¨¡å‹ç»“æ„å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œä¸»è¦åŒ…å«å››ä¸ªéƒ¨åˆ†ï¼Œä¸‰ä¸ªencoderå’Œä¸€ä¸ªAdaINï¼ˆè‡ªé€‚åº”è·ç¦»æ ‡å‡†åŒ–ï¼‰ï¼Œä½¿ç”¨è¯¥æ¨¡å—å°†éŸ³è‰²ä¿¡æ¯åŠ å…¥åˆ°éŸ³é¢‘å†…å®¹ä¿¡æ¯ä¸Šï¼Œå…¶å…¬å¼å¦‚ä¸‹æ‰€ç¤ºï¼š
  $$\mu_{c}=\frac{1}{W}\sum^{W}_{\omega=1}M_{c}[\omega],\sigma_{c}=\sqrt{\frac{1}{W}\sum^{W}_{\omega=1}(M_{c}[\omega]-\mu_{c})^{2}+\epsilon}$$
  $$M'_{c}[\omega]=\frac{M_{c}[\omega]-\mu}{\sigma_{c}}$$,cè¡¨ç¤ºç¬¬cä¸ªchannelï¼ŒWè¡¨ç¤ºæƒé‡é›†åˆï¼Œwè¡¨ç¤ºç¬¬wä¸ªæƒé‡ï¼Œæ— ä»¿å°„å˜æ¢ä¹Ÿå³åœ¨å½’ä¸€åŒ–çš„æ—¶å€™ä¸åŠ gammaä¸betaï¼ŒadaINä¸­åŠ å…¥äº†ä»¿å°„å˜æ¢ï¼ˆçº¿æ€§å˜æ¢ï¼‰
  $$M'_{c}[\omega]=\gamma_{c}\frac{M_{c}[\omega]-\mu}{\sigma_{c}}+\beta_{c}.$$
  - å…¶åœ¨encoderå’Œdecoderä¸Šéƒ½æ˜¯ä½¿ç”¨çš„ä¸€ç»´å·ç§¯å®ç°ï¼Œå…¶ä¸­speak encoderå’Œcontent encoderéƒ½ä½¿ç”¨äº†convBankï¼Œç”¨äºæ‰©å¤§æ„Ÿå—é‡ï¼Œinstance normalizationç”¨äº-   - content encoderï¼Œç”¨æ¥å»é™¤å…¨å±€é™æ€ä¿¡æ¯ï¼Œè¿™é‡Œæ ¸å¿ƒæ¨¡å—AdaINå¼•å…¥äº†speak Encoderçš„ä¿¡æ¯ã€‚
  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2111.12277.pdf
  - codeï¼šhttps://github.com/jjery2243542/adaptive_voice_conversion
  - å­¦ä¹ èµ„æ–™ï¼šhttps://zhuanlan.zhihu.com/p/166098536
- ã€ŠLPCNET: IMPROVING NEURAL SPEECH SYNTHESIS THROUGH LINEAR PREDICTION ã€‹ICASSP2019
  - 1.åœ¨WaveRNNçš„åŸºç¡€ä¸Šå¼•å…¥LPCç»“æ„ï¼Œè¯¥ç»“æ„æŠŠsource-filteréƒ¨åˆ†çš„sourceä½¿ç”¨ç¥ç»ç½‘è·¯æ¥è¿›è¡Œé¢„æµ‹ï¼Œè€Œfilteréƒ¨åˆ†åˆ™ä½¿ç”¨DSPçš„æ–¹æ³•æ¥è¿›è¡Œè®¡ç®—ï¼Œå…¶å…·ä½“è¡¨è¾¾å¼å¦‚ä¸‹æ‰€ç¤ºï¼š
  $$x_{n}=e_{n}+p_{n},p_{n}=\sum^{M}_{i}\alpha_{i}x_{n-i}$$
  - 2.å…¶ç¥ç»ç½‘ç»œåªå¯¹sourceçš„æ¿€åŠ±eè¿›è¡Œé¢„æµ‹ï¼Œè€Œfilterçš„éƒ¨åˆ†ç›´æ¥è¿›è¡Œè®¡ç®—è·å–ã€‚è¯¥æ–‡ç« çš„ä½œè€…å›ç­”è¿™æ ·çš„åšçš„åŸå› ï¼Œæ˜¯ä»–è®¤ä¸ºä½¿ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œä¸èƒ½åŒæ—¶å¾ˆå¥½çš„æ¨ç®—sourceå’Œfilterçš„ä¸¤éƒ¨åˆ†ä¿¡æ¯;
  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/1810.11846.pdf
  - codeï¼šhttps://github.com/xiph/LPCNet
  - å­¦ä¹ èµ„æ–™ï¼šhttps://zhuanlan.zhihu.com/p/321798376
- ã€ŠImproving LPCNet-based Text-to-Speech with Linear Prediction-structured Mixture Density Networkã€‹ICASSP2020
  - 1.æå‡ºåŸºäºLP-MDNçš„å£°ç å™¨ilpcnetï¼Œè¯¥å£°ç å™¨å¯ä»¥å……åˆ†åˆ©ç”¨lpå’Œæ¿€åŠ±ä¹‹é—´çš„å…³ç³»ï¼Œæ›¿æ¢åŸæ¥çš„æ¿€åŠ±u-lawç¦»æ•£åˆ†å¸ƒï¼Œä½¿åˆæˆéŸ³è´¨æé«˜;
  - 2.æå‡ºè®­ç»ƒå’Œç”Ÿæˆçš„ç­–ç•¥ï¼Œæ¯”å¦‚stft lossç­‰ç­‰ã€‚è€ƒè™‘åˆ°é‡‡æ ·ç‚¹å’Œæ¿€åŠ±ä¹‹é—´çš„åˆ†å¸ƒå…³ç³»ï¼Œæœ¬æ–‡æå‡ºLP-MDNï¼ˆmdnçš„å‡å€¼ã€æ–¹å·®ç­‰å‚æ•°æ˜¯ç½‘ç»œäº§ç”Ÿï¼‰;

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2001.11686.pdf
  - code: https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts
  - å­¦ä¹ èµ„æ–™ï¼šhttps://zhuanlan.zhihu.com/p/321798376
- ã€ŠBunched LPCNet : Vocoder for Low-cost Neural Text-To-Speech Systemsã€‹InterSpeech2020
  - 1.æ”¹è¿›LPCNeté‡‡æ ·ç­–ç•¥ï¼Œä½¿å¾—æ¨¡å‹çš„é€Ÿåº¦æå‡äº†1.5å€ï¼›
  - 2.GRUaå’ŒGRUbéƒ¨åˆ†éƒ½æ˜¯å¤šç‚¹å…±äº«ï¼ŒDual-FCå’Œsoftmaxæ˜¯æ¯ç‚¹ç‹¬äº«ã€‚ä½†æ¯ä¸ªæ¿€åŠ±ä¹‹é—´å¹¶ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œä»ç¬¬äºŒä¸ªæ¿€åŠ±å‚æ•°eå¼€å§‹ï¼Œå…¶çš„è¾“å…¥éœ€è¦å‰ä¸€ä¸ªeä¸gruè¾“å‡ºè¿›è¡Œæ‹¼æ¥ï¼Œè¯¥éƒ¨åˆ†ä¹Ÿæ˜¯ä¸€ä¸ªè‡ªå›å½’çš„æ¨¡å¼ã€‚è€Œä¸”gruAçš„è¾“å…¥ä¸å†è¾“å…¥ä¸€ä¸ªç‚¹çš„ä¿¡æ¯ï¼Œè€Œæ˜¯è¾“å…¥å¤šä¸ªç‚¹çš„ä¿¡æ¯ã€‚è¯¥ç»“æ„é€šè¿‡å…±äº«GRUéƒ¨åˆ†ï¼ŒæŒ‰ç†è¯´è¯¥éƒ¨åˆ†çš„æ¨ç†é€Ÿåº¦æ­£æ¯”äºbunch sampleçš„å¤§å°ã€‚

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2008.04574.pdf
  - codeï¼šcode
  - å­¦ä¹ èµ„æ–™ï¼šhttps://zhuanlan.zhihu.com/p/321798376
- ã€ŠGaussian Lpcnet for Multisample Speech Synthesisã€‹ICASSP2020
  - 1.ä½¿ç”¨Gaussian é‡‡æ ·æ›¿ä»£åŸæ¥çš„softmaxã€‚åŸæ¥çš„éŸ³é¢‘éœ€è¦è¿›è¡Œu-lawè½¬æ¢ï¼Œä½¿ç”¨8bitè¿›è¡Œé‡‡æ ·ç‚¹è¡¨ç¤ºï¼ˆé€ æˆéŸ³è´¨æŸå¤±ï¼‰ï¼Œåˆ™softmaxçš„ç»´åº¦åˆ™ä¸º256ã€‚æœ¬æ–‡ä½¿ç”¨gaussianç›´æ¥å¯¹16bitè¿›è¡Œé‡‡æ ·ï¼Œåˆ™é‡‡æ ·éƒ¨åˆ†ç”±åŸæ¥lpcnetçš„dualfc(256)+softmax(256)æ›¿æ¢æˆfc1(128)+fc2(2)ï¼Œæ–‡ç« æåˆ°ä¸éœ€è¦å¯¹éŸ³é¢‘è¿›è¡ŒåŠ é‡å¤„ç†ã€‚
  - 2.æœ¬æ–‡ç« è¿›è¡Œé‡‡æ ·æ—¶æ¯ä¸€æ­¥æ¨ç†ä¸¤ä¸ªé‡‡æ ·ç‚¹(ä¸¤ç‚¹ä¹‹é—´äº’ä¸å½±å“ï¼‰ï¼›

  - è®ºæ–‡ï¼šè®ºæ–‡é“¾æ¥
  - å­¦ä¹ èµ„æ–™ï¼šhttps://zhuanlan.zhihu.com/p/321798376
- ã€ŠConditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech ã€‹ICML21
  - a.æ•°æ®é¢„å¤„ç†éƒ¨åˆ†ä¼šå…ˆæ ¹æ®é¢‘è°±çš„é•¿åº¦è¿›è¡Œåˆ†æ¡¶ï¼Œè¿™é‡Œé¿å…äº†ä¸€ä¸ªbatchå†…é•¿åº¦å·®å¼‚è¾ƒå¤§å¸¦æ¥çš„è®­ç»ƒè€—æ—¶é•¿ï¼Œæ•ˆæœå·®ç­‰å½±å“ï¼Œå¦å¤–æŸ¥æ‰¾åˆ†æ¡¶çš„å·ä½¿ç”¨äº†äºŒåˆ†æŸ¥æ‰¾ä¸€å®šç¨‹åº¦ä¸ŠåŠ å¿«äº†æŸ¥æ‰¾é€Ÿåº¦ï¼›
  - b.æ¨¡å‹éƒ¨åˆ†ä½¿ç”¨çš„åŸºç¡€ç¼–ç å™¨æ˜¯transformerï¼Œå…¶æ ¸å¿ƒçš„éœ€è¦è®­ç»ƒçš„æ¨¡å—æœ‰ä¸‰éƒ¨åˆ†ï¼ˆç”Ÿæˆå™¨ã€åˆ¤åˆ«å™¨ã€æ—¶é•¿é¢„æµ‹å™¨ï¼‰ï¼Œè¿™é‡Œè·Ÿç€è§†é¢‘ï¼šhttps://www.bilibili.com/video/BV1VG411h75N/?spm_id_from=333.880.my_history.page.clickï¼Œå¯ä»¥è¿‡ä¸€éæ¨¡å‹ä¸­çš„å®ç°ç»†èŠ‚è¿˜æœ‰ä¸€äº›å…¬å¼ä¸­çš„éš¾ç‚¹ï¼›
  - c.æ¨¡å‹ä¸­è¿˜ä½¿ç”¨äº†åŠ¨æ€è§„åˆ’å»å¯»æ‰¾æœ€ä¼˜çš„è·¯å¾„ï¼Œä½¿ç”¨äº†cpythonæ¥è¿›è¡Œå¤„ç†ï¼›

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2106.06103.pdf
  - codeï¼šhttps://github.com/jaywalnut310/vits
  - å­¦ä¹ èµ„æ–™ï¼šhttps://zhuanlan.zhihu.com/p/571040094
### 22å¹´çš„æ–‡ç« 
- ã€ŠONE-SHOT VOICE CONVERSION FOR STYLE TRANSFER BASED ON SPEAKER ADAPTATIONã€‹ICASSP2022
  - è¯¥è®ºæ–‡æŒ‡å‡ºå…ˆå‰å¯¹äºéŸ³é¢‘åˆæˆå»ºæ¨¡ä¸»è¦é—®é¢˜ğŸˆ¶ï¸ä¸¤ç§æ–¹æ³•ï¼š
  - ï¼ˆ1ï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†è¯­éŸ³åˆ†è§£ä¸ºå†…å®¹å’Œè¯´è¯äººè¡¨å¾ï¼Œåœ¨æ¨æ–­é˜¶æ®µå°†éŸ³é¢‘çš„å†…å®¹è¡¨å¾å’Œç›®æ ‡è¯´è¯äººçš„è¡¨å¾è¿›è¡Œèåˆç”Ÿæˆç›®æ ‡è¯­éŸ³ï¼›è¯¥æ–¹æ³•å­¦ä¹ åˆ°çš„å†…å®¹è¡¨å¾ä¸­åŒ…å«æœ‰é£æ ¼ä¿¡æ¯ä»¥åŠè¯´è¯äººç›¸å…³ä¿¡æ¯ï¼Œä»è€Œå¯¼è‡´è½¬æ¢çš„éŸ³é¢‘ä¸å¤ªç¨³å®šï¼›
  - ï¼ˆ2ï¼‰å°†è¯­éŸ³è¡¨å¾ä¸ºå†…å®¹ã€è¯´è¯äººã€éŸ³é«˜ç­‰ï¼Œè¿™ç§æ–¹æ³•è¿›ä¸€æ­¥æå‡äº†æ¨¡å‹çš„è§£è€¦èƒ½åŠ›ï¼Œä½†æ˜¯è½¬æ¢è¯­éŸ³å’Œç›®æ ‡è¯´è¯äººè¯­éŸ³çš„è¯´è¯äººç›¸ä¼¼åº¦ä»ç„¶æœ‰å·®è·ï¼Œå¦å¤–ä¹Ÿæœ‰è¾ƒå¤šå·¥ä½œå…³æ³¨äºè¯´è¯äººè‡ªé€‚åº”æ¥åº”å¯¹few-shotç”šè‡³one-shotï¼›
  - æœ¬æ–‡çš„è§£å†³æ–¹æ³•ä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªç‚¹ï¼š
  - ï¼ˆ1ï¼‰é‡‡ç”¨è¯´è¯äººå½’ä¸€åŒ–å»é™¤ç“¶é¢ˆç‰¹å¾ä¸­è¯´è¯äººç›¸å…³çš„ä¿¡æ¯ï¼Œè¿™æ ·æœ‰åŠ©äºæé«˜æœ€ç»ˆè¯´è¯äººç›¸ä¼¼åº¦ï¼›
  - ï¼ˆ2ï¼‰å¼•å…¥æƒé‡æ­£åˆ™é˜²æ­¢æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°è¿‡æ‹Ÿåˆå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼›
  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2111.12277.pdf
  - å­¦ä¹ é“¾æ¥ï¼šhttps://zhuanlan.zhihu.com/p/617995266
- Sã€ŠSelf-supervised Context-aware Style Representation for Expressive Speech Synthesisã€‹InterSpeech 2022

  - è¯¥è®ºæ–‡æŒ‡å‡ºå½“å‰åœ¨è¯­éŸ³ä¸­å¯¹styleè¿›è¡Œå»ºæ¨¡çš„æ–¹æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼šæ— ç›‘ç£çš„joint learningå’Œæœ‰ç›‘ç£çš„è®­ç»ƒï¼Œå…¶ä¸­æ— ç›‘ç£ä¼šé¢ä¸´æ–‡æœ¬ä¿¡æ¯æ³„æ¼è¿›å…¥style encoderä¸­ï¼›å¦å¤–éœ€è¦å¤§é‡çš„éŸ³é¢‘å’Œæ–‡æœ¬ä¿¡æ¯ï¼›ä½¿ç”¨æœ‰ç›‘ç£ä¿¡æ¯ä¼šé¢ä¸´æ‰“æ ‡ç­¾æ—¶å€™å­˜åœ¨ä¸»è§‚è‡†æ–­çš„é—®é¢˜ï¼ˆæ ‡æ³¨äººå‘˜ï¼‰ï¼Œå¹¶ä¸”ä¸€ä¸ªç®€å•çš„æ ‡ç­¾å¾ˆéš¾ååº”è¯­éŸ³é£æ ¼çš„æœ¬è´¨ï¼›
æœ¬æ–‡çš„è§£å†³åŠæ³•ä¸»è¦æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š
  - ï¼ˆ1ï¼‰å¼•å…¥å¯¹æ¯”å­¦ä¹ æ¥é¢„è®­ç»ƒstyle embeddingï¼Œæ¥åŒºåˆ†ä¸åŒçš„æ–‡æœ¬çš„éŸ³è‰²ï¼Œæ›¿æ¢æ–‡æœ¬ä¸­çš„æƒ…æ„Ÿè¯è¯­ï¼Œå°†å…·æœ‰ç›¸ä¼¼æƒ…æ„Ÿçš„ä¸¤ä¸ªæ ·æœ¬ä½œä¸ºæ­£æ ·æœ¬ï¼Œä¸€ä¸ªbatchå†…çš„å…¶ä»–æ ·æœ¬ä½œä¸ºè´Ÿæ ·æœ¬;Losså¦‚ä¸‹æ‰€ç¤ºï¼š
    $$l_{cl}=-log\frac{exp(cos(h_{i},\tilde{h_{i}}))/\tau}{\sum^{N}_{k=1}\mathbf{1}_{k\neq i}exp(cos(h_{i},\tilde{h_{k}})/\tau)}$$
  - ï¼ˆ2ï¼‰å¼•å…¥ä¸€ä¸ªæ·±åº¦èšç±»Lossï¼ŒLosså¦‚ä¸‹æ‰€ç¤ºï¼š
    $$p_{ik}=\frac{q^{2}_{ik}/\sum_{i}q_{ik}}{\sum_{k'}(q^{2}_{ik'}/\sum_i q_{ik'})},Â q_{ik}=\frac{(1+||h_{i}-\mu_{k}||^{2}_{2})^{-\frac{\alpha+1}{2}}}{\sum^{K}_{k'=1}(1+||h_{i}-\mu_{k'}||^{2}_{2}/\alpha)^{-\frac{\alpha+1}{2}}}$$
    $$l_{clu}=KL(P||Q)=\sum_{i}\sum_{k}p_{ik}\log\frac{p_{ik}}{q_{ik}}$$
  - codeå®ç°å‚çœ‹ï¼šhttps://github.com/amazon-science/sccl/blob/main/training.py
  - è¿™ä¸¤ä¸ªéƒ¨åˆ†æ˜¯å‚è€ƒçš„æ–‡ç« ã€ŠSupporting Clustering with Contrastive Learningã€‹https://arxiv.org/pdf/2103.12953.pdf
  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2206.12559.pdf
  - Demoï¼šhttps://wyh2000.github.io/InterSpeech2022/
- Sã€ŠStyletts: A style-based generative model for natural and diverse text-to-speech synthesisã€‹

  - è¯¥è®ºæ–‡ä¸»è¦åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œåœ¨è®­ç»ƒé˜¶æ®µï¼Œå¼•å…¥äº†å››ä¸ªencoderå’Œä¸€ä¸ªadainæ®‹å·®æ¿å—ï¼Œè¿™é‡Œä¸»è¦å…³æ³¨çš„styleéƒ¨åˆ†ï¼Œä»–ä¸»è¦è¿˜æ˜¯åšçš„speak_to_speakçš„ä»»åŠ¡ï¼Œè¿™é‡Œstyle encoderä»melé¢‘è°±ä¸­æå–å‡ºé£æ ¼è¡¨ç¤ºï¼Œç„¶åå°†å…¶é€šè¿‡Adainæ®‹å·®æ¿å—é‡æ„melé¢‘è°±ï¼Œè¿™é‡Œçš„Losså¦‚ä¸‹æ‰€ç¤ºï¼š

  - å…¶åœ¨inferenceé˜¶æ®µè®­ç»ƒdecoerå’Œduration predictorï¼Œå…¶ç›®å‰è¿˜æ˜¯åªèƒ½å¤„ç†å•æºstyleçš„æƒ…å†µï¼Œå…¶ä½¿ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å‹è¿˜ä¼šå¢åŠ æ—¶é—´å¼€é”€ï¼›
  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2205.15439.pdf
  - ä»£ç ï¼šhttps://github.com/yl4579/StyleTTS
- Sã€ŠSTYLETTS-VC: ONE-SHOT VOICE CONVERSION BY KNOWLEDGE TRANSFER FROM STYLE-BASED TTS MODELSã€‹SLT2022

  - ä¸Šä¸€ç¯‡è®ºæ–‡çš„æ”¹è¿›ç‰ˆï¼Œå…¶ä¸»è¦ç”¨äºå•ä¸€æ ·æœ¬çš„éŸ³é¢‘åˆæˆ
  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2212.14227.pdf
  - codeï¼šGitHub - yl4579/StyleTTS-VC: Official Implementation of StyleTTS-VC
  - demoï¼šhttps://styletts-vc.github.io/
- ã€ŠFastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis ã€‹IJCAI2022

  - åŠ å…¥Time-Aware LVCæ¨¡å—ï¼Œç”¨äºæå‡éŸ³é¢‘åˆæˆé€Ÿåº¦
  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2204.09934.pdf
  - codeï¼šhttps://github.com/Rongjiehuang/FastDiff
  - demoï¼šhttps://fastdiff.github.io/
- ã€ŠHierSpeech: Bridging the Gap between Text and Speech by Hierarchical Variational Inference using Self-supervised Representations for Speech Synthesis ã€‹ NeurIPS2022

  - å¼•å…¥å…ˆéªŒçŸ¥è¯†ï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹Wave2vec2.0æ¥æå–ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨KLæŸå¤±æ¥è¿›è¡Œçº¦æŸï¼Œè¿™é‡Œå…¶ä»–éƒ¨åˆ†ä¸vitsä¸€æ ·
  - è®ºæ–‡ï¼šhttps://openreview.net/pdf?id=awdyRVnfQKX
  - ä½œè€…ä¸»é¡µï¼šhttps://github.com/sh-lee-prml
  - demoï¼šhttps://sh-lee-prml.github.io/hierspeech-demo/
- ã€ŠGlow-WaveGAN 2: High-quality Zero-shot Text-to-speech Synthesis and Any-to-any Voice Conversion ã€‹InterSpeech2022
  - ä¸»è¦å…³æ³¨é›¶æ ·æœ¬éŸ³é¢‘åˆæˆ
  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2207.01832.pdf
  - å­¦ä¹ é“¾æ¥ï¼šhttps://zhuanlan.zhihu.com/p/547672526
  - demoï¼šhttps://leiyi420.github.io/glow-wavegan2/
- ã€ŠA Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural TTS ã€‹InterSpeech 2022

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2209.10887.pdf
  - codeï¼šhttps://github.com/NVIDIA/NeMo
- Sã€ŠVStyclone: Real-time Chinese voice style clone ã€‹2022

  - è¿™ç¯‡æ–‡ç« æå–é£æ ¼è¡¨ç¤ºçš„æ–¹æ³•ä¸GSTä¸€æ ·ï¼Œä½†æ˜¯åœ¨æå–çš„åœ°æ–¹å’Œåˆæˆçš„åœ°æ–¹æœ‰å·®å¼‚ï¼›
  - è®ºæ–‡ï¼šè®ºæ–‡é“¾æ¥
  - codeï¼šhttps://github.com/babysor/MockingBird
- Sã€ŠGenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speechã€‹NeurIPS 2022

  - è®ºæ–‡ï¼šè®ºæ–‡é“¾æ¥ 
  - PPTï¼šhttps://neurips.cc/media/neurips-2022/Slides/54425.pdf
  - demoï¼šhttps://generspeech.github.io/
- Sã€ŠDisentangling Style and Speaker Attributes for TTS Style Transferã€‹ASP 2022

  - è¯¥è®ºæ–‡åˆ›æ–°çš„åœ°æ–¹åœ¨äºä¸ºstyleå•ç‹¬è®¾ç½®äº†ä¸€ä¸ªåˆ¤åˆ«å™¨ç”¨äºä¸“é—¨å­¦ä¹ è¯¥éŸ³é¢‘çš„style
  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2201.09472.pdf
  - demoï¼šhttps://xiaochunan.github.io/disentangling/index.html
- Sã€ŠFINE-GRAINED STYLE CONTROL IN TRANSFORMER-BASED TEXT-TO-SPEECH SYNTHESISã€‹ICASSP 2022

  - è¯¥è®ºæ–‡ä½¿ç”¨äº†é¢„è®­ç»ƒæ¨¡å‹Wave2Vecæ¥æå–éŸ³é¢‘é£æ ¼å’Œå…¶ä»–ç‰¹å¾ï¼Œé€šè¿‡GSTå’ŒLSTæ¨¡å—æ¥æŠ½å–æœ€ç»ˆstyleè¡¨ç¤ºï¼›
  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2110.06306.pdf
  - ä»£ç ï¼šhttps://github.com/b04901014/FG-transformer-TTS
  - demoï¼šhttps://b04901014.github.io/FG-transformer-TTS/
- Sã€ŠCROSS-SPEAKER STYLE TRANSFER FOR TEXT-TO-SPEECH USING DATA AUGMENTATIONã€‹
  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2202.05083.pdf
### 23å¹´çš„æ–‡ç« 
- Sã€ŠIMPROVING THE QUALITY OF NEURAL TTS USING LONG-FORM CONTENT AND MULTI-SPEAKER MULTI-STYLE MODELINGã€‹ICASSP 2023
  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2212.10075.pdf
- ã€ŠCONTEXTUAL EXPRESSIVE TEXT-TO-SPEECHã€‹ICASSP2023
  
  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2211.14548.pdf
  - demoï¼šhttps://ofa-sys.github.io/Demo_CTTS/
- Sã€ŠIMPROVING PROSODY FOR CROSS-SPEAKER STYLE TRANSFER BY SEMI-SUPERVISED STYLE EXTRACTOR AND HIERARCHICAL MODELING IN SPEECH SYNTHESISã€‹ICASSP 2023

  - è¯¥è®ºæ–‡ä½¿ç”¨åŠç›‘ç£çš„æ–¹æ³•æ¥å­¦ä¹ styleï¼Œå…¶ä½¿ç”¨äº†ä¸€éƒ¨åˆ†æ ‡æ³¨styleçš„æ•°æ®æ··åˆæœªæ ‡æ³¨çš„æ•°æ®ä¸€èµ·è®­ç»ƒæ¨¡å‹
  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2303.07711.pdf
  - Demoï¼šhttps://qiangchunyu.github.io/style-transfer/STW.html
- ã€ŠDURATION-AWARE PAUSE INSERTION USING PRE-TRAINED LANGUAGE MODEL FOR MULTI-SPEAKER TEXT-TO-SPEECHã€‹ICASSP 2023

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2302.13652.pdf
  - demoï¼šhttps://ydqmkkx.github.io/pause-insertion/
- ã€ŠTEXT-TO-SPEECH SYNTHESIS BASED ON LATENT VARIABLE CONVERSION USING DIFFUSION PROBABILISTIC MODEL AND VARIATIONAL AUTOENCODER ã€‹ICASSP2023
  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2212.08329.pdf
- ã€ŠEVALUATING AND REDUCING THE DISTANCE BETWEEN SYNTHETIC AND REAL SPEECH DISTRIBUTIONSã€‹ICASSP2023
  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2211.16049.pdf
  - codeï¼šhttps://github.com/MiniXC/LightningFastSpeech2
- Sã€ŠGRAD-STYLESPEECH: ANY-SPEAKER ADAPTIVE TEXT-TO-SPEECH SYNTHESIS WITH DIFFUSION MODELS ã€‹ICASSP2023

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2211.09383.pdf
  - demoï¼šhttps://nardien.github.io/grad-stylespeech-demo/
- ã€ŠPERIOD VITS: VARIATIONAL INFERENCE WITH EXPLICIT PITCH MODELING FOR END-TO-END EMOTIONAL SPEECH SYNTHESIS ã€‹ICASSP2023

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2210.15964.pdf
  - demoï¼šhttps://yshira116.github.io/period_vits_demo/
- ã€ŠLIGHTWEIGHT AND HIGH-FIDELITY END-TO-END TEXT-TO-SPEECH WITH MULTI-BAND GENERATION AND INVERSE SHORT-TIME FOURIER TRANSFORM ã€‹ICASSP2023

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2210.15975.pdf
  - codeï¼šhttps://github.com/MasayaKawamura/MB-iSTFT-VITS
  - demoï¼šhttps://masayakawamura.github.io/Demo_MB-iSTFT-VITS/
- ã€ŠCROSSSPEECH: SPEAKER-INDEPENDENT ACOUSTIC REPRESENTATION FOR CROSS-LINGUAL SPEECH SYNTHESIS ã€‹ICASSP2023

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2302.14370.pdf
  - demoï¼šhttps://lism13.github.io/demo/CrossSpeech/
- ã€ŠPROSODY-TTS: SELF-SUPERVISED PROSODY PRE- TRAINING WITH LATENT DIFFUSION FOR TEXT-TO- SPEECH ã€‹NeurIPS2023 under view

  - è®ºæ–‡ï¼šopenreview.net
- ã€ŠEND-TO-END SPEECH SYNTHESIS BASED ON DEEP CONDITIONAL SCHRO Ìˆ DINGER BRIDGES ã€‹NeurIPS2023 under view

  - è®ºæ–‡ï¼šhttps://openreview.net/pdf?id=K7YxdCYmd6w
  - demoï¼šhttps://schron.github.io/
- ã€ŠEFFICIENTTTS 2: VARIATIONAL END-TO-END TEXT- TO-SPEECH SYNTHESIS AND VOICE CONVERSION ã€‹NeurIPS2023 under view

  - è®ºæ–‡ï¼šhttps://openreview.net/pdf?id=__czv_gqDQt
- ã€ŠA Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech ã€‹ AAAI2023

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2302.04215.pdf
  - codeï¼šhttps://github.com/b04901014/MQTTS
- ã€ŠRWEN-TTS: Relation-aware Word Encoding Network for Natural Text-to-Speech Synthesis ã€‹AAAI2023

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2212.07939.pdf
  - codeï¼šhttps://github.com/shinhyeokoh/rwen
- ã€ŠDINOISER: Diffused Conditional Sequence Learning by Manipulating Noises ã€‹2023
  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2302.10025.pdf
  - codeï¼šhttps://github.com/yegcjs/DINOISER
- ã€ŠFoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model ã€‹2023

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2303.02939.pdf
- ã€ŠLEVERAGING LARGE TEXT CORPORA FOR END-TO-END SPEECH SUMMARIZATION ã€‹2023

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2303.00978.pdf
- ã€ŠUniFLG: Unified Facial Landmark Generator from Text or Speech ã€‹2023

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2302.14337.pdf
  - demoï¼šhttps://rinnakk.github.io/research/publications/UniFLG/
- ã€ŠPITS: Variational Pitch Inference without Fundamental Frequency for End-to-End Pitch-controllable TTS ã€‹2023

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2302.12391.pdf
  - codeï¼šhttps://github.com/anonymous-pits/pits
- ã€ŠQuickVC: Any-To-Many Voice Conversion Using Inverse Short-Time Fourier Transform for Faster Conversion ã€‹2023

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2302.08296.pdf
  - codeï¼šhttps://github.com/quickvc/QuickVC-VoiceConversion
  - demoï¼šhttps://ericwudayi.github.io/VQVC-DEMO/
- *Sã€ŠInstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt ã€‹2023

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2301.13662.pdf
  - demoï¼šhttp://dongchaoyang.top/InstructTTS/
  - codeï¼šhttps://github.com/yangdongchao/InstructTTS
  - å‚è€ƒcodeï¼šhttps://github.com/yangdongchao/Text-to-sound-Synthesis
- ã€ŠNeural Codec Language Models are Zero-Shot Text to Speech Synthesizers ã€‹2023

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2301.02111.pdf
  - demoï¼šhttps://valle-demo.github.io/
  - codeï¼šhttps://github.com/microsoft/unilm
- ã€ŠLearning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining ã€‹2023

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2301.12596.pdf
  - demoï¼šhttps://takaaki-saeki.github.io/zm-tts-text_demo/
- ã€ŠControllable and Lossless Non-Autoregressive End-to-End Text-to-Speech ã€‹2023

  - è®ºæ–‡ï¼šhttps://arxiv.org/pdf/2207.06088.pdf
  - demoï¼šhttps://xcmyz.github.io/CLONE/

### å…¶ä»–èµ„æº
- ç«¯åˆ°ç«¯TTSæ¨¡å‹: https://github.com/keonlee9420/Comprehensive-E2E-TTS
- TTSPaperï¼šhttps://github.com/RevoSpeechTech/audio-generation-papers
- LM Audioï¼šhttps://github.com/liusongxiang/Large-Audio-Models
- New TTS: https://github.com/wenet-e2e/speech-synthesis-paper
